{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Material: https://drive.google.com/drive/folders/1L88jOg5GP7k17XTOHliYTWNy2qo-hHLX?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1.PNG'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-97e3be6fc99b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1.PNG'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda2/envs/lighthouse-lab-course/lib/python3.8/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0m\u001b[1;32m   1225\u001b[0m                 metadata=metadata)\n\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/lighthouse-lab-course/lib/python3.8/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/lighthouse-lab-course/lib/python3.8/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1254\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/lighthouse-lab-course/lib/python3.8/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1.PNG'"
     ]
    }
   ],
   "source": [
    "Image(filename='1.PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='2.PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import load_boston\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# helpful functions for later\n",
    "\n",
    "def plot_reduction2d(data, w, h, drop):\n",
    "\n",
    "    dimension = set([0, 1])\n",
    "    dimension = list(dimension.difference([drop]))[0]\n",
    "\n",
    "    x_std = data.to_numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(w, h))\n",
    "    ax[0].scatter(x_std[:, 0], x_std[:, 1], zorder=2)  # Plot the points\n",
    "\n",
    "    # This for will draw the red lines into the plot\n",
    "    for i in range(0, x_std.shape[0]):\n",
    "        if dimension == 0:\n",
    "            ax[0].plot([x_std[i, 0], x_std[i, 0]], [0, x_std[i, 1]],\n",
    "                       color=\"red\", linestyle=\"-.\", alpha=.4, zorder=1)\n",
    "        if dimension == 1:\n",
    "            ax[0].plot([0, x_std[i, 0]],\n",
    "                       [x_std[i, 1], x_std[i, 1]],\n",
    "                       color=\"red\", linestyle=\"-.\", alpha=.4, zorder=1)\n",
    "\n",
    "    # This line plots the black line\n",
    "    if dimension == 0:\n",
    "        ax[0].plot([np.min(x_std[:, 0]),\n",
    "                    np.max(x_std[:, 0])],\n",
    "                   [0, 0], color=\"black\", zorder=0)\n",
    "    if dimension == 1:\n",
    "        ax[0].plot([0, 0], [np.min(x_std[:, 1]), np.max(x_std[:, 1])],\n",
    "                   color=\"black\", zorder=0)\n",
    "\n",
    "    # This line plots the red points\n",
    "    if dimension == 0:\n",
    "        ax[0].scatter(x_std[:, 0], np.zeros_like(x_std[:, 0]), color=\"red\")\n",
    "    if dimension == 1:\n",
    "        ax[0].scatter(np.zeros_like(x_std[:, 1]), x_std[:, 1], color=\"red\")\n",
    "\n",
    "    # Editing frame\n",
    "    ax[0].spines['top'].set_visible(False)\n",
    "    ax[0].spines['right'].set_visible(False)\n",
    "\n",
    "    # Increases the size of the labels' ticks;\n",
    "    ax[0].set_xticklabels(ax[0].get_xticks(), fontdict={'fontsize': 16})\n",
    "    ax[0].set_yticklabels(ax[0].get_yticks(), fontdict={'fontsize': 16})\n",
    "\n",
    "    # Editing labels\n",
    "    ax[0].set_xlabel(data.columns[0], fontdict={'fontsize': w})\n",
    "    ax[0].set_ylabel(data.columns[1], fontdict={'fontsize': w})\n",
    "    ax[0].set_title(\"Dropping the column $X_\" + str(drop+1) +\n",
    "                    \"$ = \" + data.columns[drop], fontsize=w+h)\n",
    "\n",
    "    # The functions called here are the same as above,\n",
    "    # but this time is for the plot on the right\n",
    "    ax[1].scatter(x_std[:, dimension], np.zeros_like(x_std[:, 0]), color=\"red\")\n",
    "    ax[1].spines['left'].set_visible(False)\n",
    "    ax[1].spines['right'].set_visible(False)\n",
    "    ax[1].spines['top'].set_visible(False)\n",
    "    ax[1].set_title(\"Projected points\", fontsize=w+h)\n",
    "    ax[1].set_yticks([])\n",
    "    ax[1].set_xticklabels(ax[1].get_xticks(), fontdict={'fontsize': 16})\n",
    "    ax[1].set_xlabel(\"$Z_1 = $\" + data.columns[dimension],\n",
    "                     fontdict={'fontsize': w})\n",
    "    \n",
    "def plot_rotation2d(data, w, h, theta, projection=True, projection_line=True, plot_error=True, show_error=False, clear=False):\n",
    "    \"\"\"\n",
    "    Plot different lines for projections;\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    w: width of the figure\n",
    "    h: height of the figure\n",
    "    theta: angle (in degrees) of the line\n",
    "    clear: if true the result of the cell will be cleared before plotting;\n",
    "    \"\"\"\n",
    "\n",
    "    x_std = data.to_numpy()\n",
    "\n",
    "    theta = np.deg2rad(theta)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(w, h))\n",
    "\n",
    "    # Plot the points\n",
    "    ax[0].scatter(x_std[:, 0], x_std[:, 1])\n",
    "\n",
    "    r = [[np.cos(theta)], [np.sin(theta)]] / \\\n",
    "        np.linalg.norm([np.cos(theta), np.sin(theta)])\n",
    "    z = x_std[:, 0:2] @ r\n",
    "    v = z @ r.reshape(1, -1)\n",
    "    error = np.round(np.sum((x_std[:, 0:2] - v)**2), 2)\n",
    "    \n",
    "    # This line plots the black line\n",
    "    if projection_line:\n",
    "        if np.rad2deg(theta) <= 90:\n",
    "            ax[0].plot([np.min(v[:, 0]), np.max(v[:, 0])],\n",
    "                       [np.min(v[:, 1]), np.max(v[:, 1])], color=\"black\")\n",
    "        else:\n",
    "            ax[0].plot([np.min(v[:, 0]), np.max(v[:, 0])],\n",
    "                       [np.max(v[:, 1]), np.min(v[:, 1])], color=\"black\")\n",
    "\n",
    "        \n",
    "    \n",
    "    if projection:\n",
    "        # This for will draw the red lines into the plot\n",
    "        if plot_error:\n",
    "            for i in range(0, x_std.shape[0]):\n",
    "                ax[0].plot([x_std[i, 0], v[i, 0]], [x_std[i, 1], v[i, 1]],\n",
    "                           color=\"red\", linestyle=\"-.\", alpha=.4)\n",
    "        \n",
    "        # This line plots the red points\n",
    "        ax[0].scatter(v[:, 0], v[:, 1], color=\"red\")\n",
    "\n",
    "    # Removing frame sides\n",
    "    ax[0].spines['top'].set_visible(False)\n",
    "    ax[0].spines['right'].set_visible(False)\n",
    "\n",
    "    # Increasing the size of the ticks labels;\n",
    "    ax[0].set_xticklabels(ax[0].get_xticks(), fontdict={'fontsize': 16})\n",
    "    ax[0].set_yticklabels(ax[0].get_yticks(), fontdict={'fontsize': 16})\n",
    "\n",
    "    # Changes the labels\n",
    "    ax[0].set_xlabel(data.columns[0], fontdict={'fontsize': w})\n",
    "    ax[0].set_ylabel(data.columns[1], fontdict={'fontsize': w})\n",
    "    if projection and show_error:\n",
    "        ax[0].set_title(f\"Original space (reconstruction error: {error})\", fontsize=w)\n",
    "    else:\n",
    "        ax[0].set_title(f\"Projecting points\", fontsize=w)\n",
    "\n",
    "    ax[1].scatter(z, np.zeros_like(z), color=\"red\")\n",
    "    ax[1].spines['left'].set_visible(False)\n",
    "    ax[1].spines['right'].set_visible(False)\n",
    "    ax[1].spines['top'].set_visible(False)\n",
    "    var = np.round(np.var(z), 4)\n",
    "\n",
    "    # Define the title of the plot\n",
    "    ax[1].set_title(f\"Projected points (var: {var})\", fontsize=w)\n",
    "    ax[1].set_yticks([])  # Here it is just removing the ticks from the y-axis\n",
    "    ax[1].set_xticklabels(ax[1].get_xticks(), fontdict={'fontsize': 16})\n",
    "    ax[1].set_xlabel(\"$Z_1$\", fontdict={'fontsize': w})\n",
    "    if clear:\n",
    "        clear_output()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def plot_pca_rotation2d(data, w, h):\n",
    "\n",
    "    x_std = data.iloc[:, 0:2].to_numpy()\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    z = pca.fit_transform(x_std)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(w, h))\n",
    "    ax[0].scatter(x_std[:, 0], x_std[:, 1], alpha=.4)\n",
    "    ax[0].axis(\"equal\")\n",
    "    ax[0].quiver(pca.components_[0, 0], pca.components_[0, 1],\n",
    "                 color=\"black\", label=\"Direction 1nd PC\")\n",
    "    ax[0].quiver(pca.components_[1, 0], pca.components_[\n",
    "                 1, 1], color=\"red\", label=\"Direction 2nd PC\")\n",
    "    ax[0].spines['top'].set_visible(False)  # Removes the top side of the frame\n",
    "    # Removes the right side of the frame\n",
    "    ax[0].spines['right'].set_visible(False)\n",
    "    # Increase the size of the x-axis ticks labels;\n",
    "    ax[0].set_xticklabels(ax[0].get_xticks(), fontdict={'fontsize': 16})\n",
    "    # Increase the size of the y-axis ticks labels;\n",
    "    ax[0].set_yticklabels(ax[0].get_yticks(), fontdict={'fontsize': 16})\n",
    "    # Define the title of the plot\n",
    "    ax[0].set_title(f\"Original data\", fontsize=w+h)\n",
    "    ax[0].legend(fontsize=w, loc=\"lower right\")\n",
    "    ax[0].set_xlabel(\"$X_1$\")\n",
    "    ax[0].set_ylabel(\"$X_2$\")\n",
    "\n",
    "    ax[1].scatter(z[:, 0], z[:, 1])\n",
    "    ax[1].axis(\"equal\")\n",
    "    ax[1].spines['top'].set_visible(False)  # Removes the top side of the frame\n",
    "    # Removes the right side of the frame\n",
    "    ax[1].spines['right'].set_visible(False)\n",
    "    # Increase the size of the x-axis ticks labels;\n",
    "    ax[1].set_xticklabels(ax[1].get_xticks(), fontdict={'fontsize': 16})\n",
    "    # Increase the size of the y-axis ticks labels;\n",
    "    ax[1].set_yticklabels(np.round(ax[1].get_yticks(), 2),\n",
    "                          fontdict={'fontsize': 16})\n",
    "    # Define the title of the plot\n",
    "    ax[1].set_title(f\"Transformed data\", fontsize=w+h)\n",
    "    ax[1].set_xlabel(\"$Z_1$\")\n",
    "    ax[1].set_ylabel(\"$Z_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is dimensionality reduction?\n",
    "- Reducing the number of features in a dataset\n",
    "- E.g., 1000 rows by 20 columns (features) to 1000 rows by 10 columns\n",
    "\n",
    "\n",
    "## Why do we do it?\n",
    "- Helps our machine learning algorithms perform better\n",
    "- Improves run-time of our algorithms\n",
    "- Storing and using less data (memory)\n",
    "- For visualization\n",
    "\n",
    "**The best solution is the most parsimonious model with acceptable accuracy (or other metric).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When do we do dimensionality reduction?\n",
    "- Before visualization\n",
    "- To improve the performance of our baseline model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "**The goal is to preserve as much of the important data as possible.**\n",
    "\n",
    "Two well-known techniques amongst many others that we'll cover today:\n",
    "\n",
    "- Principal Component Analysis (PCA)\n",
    "- Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "- Consider a data matrix $X$ with $n$ rows and $d$ columns\n",
    "\n",
    "\n",
    "- We want a new data matrix $Z$ with $n$ rows and $k\\leq d$ columns\n",
    "\n",
    "$$\n",
    "\\textbf{X} = \n",
    "\\underbrace{\\left[\n",
    "  \\begin{array}{cccc}\n",
    "    \\rule[-1ex]{0.5pt}{2.5ex} & \\rule[-1ex]{0.5pt}{2.5ex} &   &  &   & \\rule[-1ex]{0.5pt}{2.5ex} \\\\\n",
    "    \\textbf{X}_{1}    & \\textbf{X}_{2}    & \\ldots & \\ldots & \\ldots & \\textbf{X}_{d}    \\\\\n",
    "    \\rule[-1ex]{0.5pt}{2.5ex} & \\rule[-1ex]{0.5pt}{2.5ex} &   &   &  & \\rule[-1ex]{0.5pt}{2.5ex} \n",
    "  \\end{array}\n",
    "\\right]}_{\\text{d columns (wider)}}\\\\\n",
    "\\textbf{Z} = \n",
    "\\underbrace{\\left[\n",
    "  \\begin{array}{cccc}\n",
    "    \\rule[-1ex]{0.5pt}{2.5ex} &         & \\rule[-1ex]{0.5pt}{2.5ex} \\\\\n",
    "    {\\textbf{Z}}_{1}    & \\ldots & \\textbf{Z}_{k}    \\\\\n",
    "    \\rule[-1ex]{0.5pt}{2.5ex}  &        & \\rule[-1ex]{0.5pt}{2.5ex} \n",
    "  \\end{array}\n",
    "\\right]}_{\\text{k columns (narrower)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 1: Let's load in some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "wine_data = datasets.load_wine() #load dataset\n",
    "wine = pd.DataFrame(wine_data.data, columns=wine_data['feature_names']) # turn into dataframe\n",
    "X = StandardScaler().fit_transform(wine) # scaling\n",
    "wine_std = pd.DataFrame(X, columns = wine.columns) # put back in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_std.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Concept 1: Projecting data to reduce number of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reduction2d(wine_std[['alcohol', 'magnesium']], 15, 8, drop = 0) # can try to drop 1 too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Concept 2: Choosing a line to project on\n",
    "- In PCA, we're trying to project onto an axis of maximum variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for θ in np.linspace(0,170, 8):\n",
    "    plot_rotation2d(wine_std, 15, 8, θ, clear=True)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Concept 3: Many components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_rotation2d(wine_std[['alcohol', 'malic_acid']], 15, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mathematically speaking ...\n",
    "\n",
    "Let $Z$ be our new desired representation.\n",
    "\n",
    "$$\n",
    "\\textbf{Z}_{(n\\times k)} = \\textbf{X}_{(n\\times d)}\\left(\\textbf{w}_{(k\\times d)}\\right)^T\n",
    "$$\n",
    "\n",
    "where $1\\leq k \\leq d$ and $d$ is the dimension of the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "    \\rule[-1ex]{0.5pt}{2.5ex} &         & \\rule[-1ex]{0.5pt}{2.5ex} \\\\\n",
    "    {\\textbf{Z}}_{1}    & \\ldots & \\textbf{Z}_{d}    \\\\\n",
    "    \\rule[-1ex]{0.5pt}{2.5ex}  &        & \\rule[-1ex]{0.5pt}{2.5ex} \n",
    "  \\end{array}\n",
    "\\right] = \n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "    \\rule[-1ex]{0.5pt}{2.5ex} & \\rule[-1ex]{0.5pt}{2.5ex}    &     & \\rule[-1ex]{0.5pt}{2.5ex} \\\\\n",
    "    \\textbf{X}_{1}    & \\textbf{X}_{2}    & \\ldots  & \\textbf{X}_{d}    \\\\\n",
    "    \\rule[-1ex]{0.5pt}{2.5ex} & \\rule[-1ex]{0.5pt}{2.5ex}    &     & \\rule[-1ex]{0.5pt}{2.5ex} \n",
    "  \\end{array}\n",
    "\\right]\\underbrace{\\left[\n",
    "  \\begin{array}{cccc}\n",
    "    \\rule[-1ex]{0.5pt}{2.5ex} & \\rule[-1ex]{0.5pt}{2.5ex} &        & \\rule[-1ex]{0.5pt}{2.5ex} \\\\\n",
    "    \\textbf{PC}_{1}    & \\textbf{PC}_{2}    & \\ldots  & \\textbf{PC}_{d}    \\\\\n",
    "    \\rule[-1ex]{0.5pt}{2.5ex} & \\rule[-1ex]{0.5pt}{2.5ex} &        & \\rule[-1ex]{0.5pt}{2.5ex} \n",
    "  \\end{array}\n",
    "\\right]}_{\\left(\\textbf{w}_{(d\\times d)}\\right)^T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Each PC is an eigenvector, this is why we learned about this concept in LinAlg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The code\n",
    "- Let's just use the first 3 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wine_std.iloc[:, 0:3].to_numpy()\n",
    "X[0:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's fit a PCA, turning our $X$ into $Z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA object will fit a PCA for us\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# Learning the components and transforming X into Z\n",
    "Z = pca.fit_transform(X)\n",
    "\n",
    "print(Z[0:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is our $w$? They are our components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our w (the components row-wise)\n",
    "w = pca.components_\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's make sure it checks out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check:\n",
    "print(Z[0:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(X,w.T)[0:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The whole point is dimensionality *reduction*\n",
    "- How many components do I pick?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance explained\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting PCA on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA on the wine dataset\n",
    "pca_wine = PCA(n_components=13)\n",
    "pca_wine.fit(wine_std) # notice I only do fit here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Plotting variance explained - Elbow Plot\n",
    "- How many components do I use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_wine.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the proportion of variance explained\n",
    "var_exp_wine = np.cumsum(pca_wine.explained_variance_ratio_)\n",
    "\n",
    "# Plots the elbow plot for the wine pca\n",
    "plt.plot(range(1,14), var_exp_wine, marker='o')\n",
    "plt.title(\"Variance explained - Wine\", fontsize=18);\n",
    "plt.axhline(0.9,c='r')\n",
    "plt.axvline(7,c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Using PCA on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape # 8 X 8 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "print('Projecting %d-dimensional data to 2D' % X.shape[1])\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, \n",
    "            edgecolor='none', alpha=0.7, s=40,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "im = pca.components_[0]\n",
    "ax1.imshow(im.reshape((8, 8)), cmap='binary')\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_title('First principal component')\n",
    "\n",
    "im = pca.components_[1]\n",
    "ax2.imshow(im.reshape((8, 8)), cmap='binary')\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "ax2.set_title('Second principal component')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[15].reshape((8, 8)), cmap='binary')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Source image')\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    pca = PCA(i + 1).fit(X)\n",
    "    im = pca.inverse_transform(pca.transform(X[15].reshape(1, -1)))\n",
    "\n",
    "    ax.imshow(im.reshape((8, 8)), cmap='binary')\n",
    "    ax.text(0.95, 0.05, 'n = {0}'.format(i + 1), ha='right',\n",
    "            transform=ax.transAxes, color='red')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(X)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), color='k', lw=2)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Total explained variance')\n",
    "plt.xlim(0, 63)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.axvline(28, c='b')\n",
    "plt.axhline(0.95, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.95).fit(X)\n",
    "print('We need %d components to explain 95%% of variance' \n",
    "      % pca.n_components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Discriminant Analysis (LDA)\n",
    "- Similar to PCA: Projecting onto smaller number of dimensions\n",
    "- Different from PCA: Uses the `y` or class label to help us decide what to select\n",
    "- Can only use for classification (remember, when `y` is discrete)\n",
    "\n",
    "<img src='imgs/pca-v-lda.png' width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducing the Iris dataset\n",
    "\n",
    "<img src='imgs/iris-dataset.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LDA: concept of interclass variance\n",
    "- Which features better delineates the classes?\n",
    "\n",
    "<img src='imgs/lda-iris.png' width=800>\n",
    "\n",
    "**We are trying to find components that minimize the intra-class variance and maximizes the inter-class variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The code: import iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {i:label for i,label in zip(\n",
    "                range(4),\n",
    "                  ('sepal length in cm',\n",
    "                  'sepal width in cm',\n",
    "                  'petal length in cm',\n",
    "                  'petal width in cm', ))}\n",
    "\n",
    "df = pd.io.parsers.read_csv(\n",
    "    filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n",
    "    header=None,\n",
    "    sep=',',\n",
    "    )\n",
    "df.columns = [l for i,l in sorted(feature_dict.items())] + ['class label']\n",
    "df.dropna(how=\"all\", inplace=True) # to drop the empty line at file-end\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The code: some data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['class label'], axis=1).values\n",
    "y = df['class label'].values\n",
    "\n",
    "# creating dummy variables\n",
    "enc = LabelEncoder()\n",
    "label_encoder = enc.fit(y)\n",
    "y = label_encoder.transform(y) + 1\n",
    "\n",
    "label_dict = {1: 'Setosa', 2: 'Versicolor', 3:'Virginica'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The code: LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "lda = LDA(n_components=2)\n",
    "X_lda = lda.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original X Shape:',X.shape)\n",
    "print('Transformed shape:',X_lda.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA vs. LDA comparison for iris\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-lda-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variable Selection\n",
    "- PCA and LDA are sophisticated ways to try to \"pack more data into less dimensions\"\n",
    "- The challenge is ***explainability***: we no longer really know what the dimensions represent\n",
    "\n",
    "### Why variable selection?\n",
    "- Other variable selection techniques exist to maintain explainability while downselecting variables\n",
    "- They require less computational time and may perform just as well\n",
    "- A simple example is just picking the best 10 out of the 20 features that are available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Back to our old example\n",
    "- What would be the features we would pick for the `iris` dataset if we wanted to make a classification model?\n",
    "\n",
    "<img src='imgs/lda-iris.png' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variable Selection Techniques\n",
    "\n",
    "**Filter methods**\n",
    "   - Using correlation and other metrics to select variables during data prep\n",
    "   \n",
    "**Wrapper methods**\n",
    "   - \"Try (on a model), measure and try (fitting the model) again\"\n",
    "   - Techniques like backward selection, forward selection\n",
    "   \n",
    "**Embedded methods (will cover on Friday)**\n",
    "   - Altering the algorithm itself to weed out variables that aren't useful\n",
    "    \n",
    "Usually some combination of all three are used / tried over the entire machine learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variable Selection - Filter Method Example\n",
    "- Remember our Boston Housing dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_housing  = load_boston()\n",
    "boston_df = pd.DataFrame(boston_housing.data, columns=boston_housing.feature_names)\n",
    "boston_df['target'] = boston_housing.target\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using correlation\n",
    "- Looking for variables that are highly correlated with target - **Will help us in our regression task**\n",
    "- Eliminating variables that are highly correlated with each other - **Don't really add more new information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get pair-wise correlation between features and features and target\n",
    "cor = boston_df.corr()\n",
    "cor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Correlation heat map\n",
    "- Much easier to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Drawbacks of using correlation\n",
    "\n",
    "- Ignores variable interactions\n",
    "- Looking at feature combinations independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variable Selection - Wrapper Method\n",
    "- Remember that wrapper method uses a \"try and see\" approach for a given model\n",
    "- **Forward Selection**: Start with having no features in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.\n",
    "- **Backward Elimination**: Start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variable Selection - Wrapper Method continued...\n",
    "### Recursive Feature Elimination\n",
    "\n",
    "1. Decide $k$, the number of features to select. \n",
    "* Use a model (usually a linear model) to assign weights to features.\n",
    "    - The weights of important features have higher absolute value.\n",
    "* Rank the features based on the absolute value of weights.\n",
    "* Drop the least useful feature.\n",
    "* Try steps 2-4 again until desired number of features is reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variable Selection - Wrapper Methods Tips\n",
    "- Look for implementations, `sklearn` has a `rfe` implementations, for example\n",
    "- It's not possible to tell which method will work better until you try\n",
    "- Different variable selection algorithms may give you a different answers\n",
    "- Different machine learning algorithms with the same variable selection method may give you given answers\n",
    "- Over this process, you'll find out what features tend to get eliminated and which features tend to be kept (hopefully)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
