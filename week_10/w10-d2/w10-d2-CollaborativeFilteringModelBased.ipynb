{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnnscrapercondaa354d8fb32614818906b42d96b0e4fbd",
   "display_name": "Python 3.8.5 64-bit ('NNScraper': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Collaborative Filtering Model Based"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = '/home/henri/Documents/Lighthouse-lab/Databases/w10-d2-db/filmtrust/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import io\n",
    "import zipfile\n",
    "\n",
    "dataset = pd.read_table(url + 'ratings.txt', sep= ' ', names = ['uid','iid','rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   uid  iid  rating\n",
       "0    1    1     2.0\n",
       "1    1    2     4.0\n",
       "2    1    3     3.5\n",
       "3    1    4     3.0\n",
       "4    1    5     4.0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>uid</th>\n      <th>iid</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>3</td>\n      <td>3.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>4</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>5</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.5 4.0\n"
     ]
    }
   ],
   "source": [
    "lower_rating = dataset['rating'].min()\n",
    "upper_rating = dataset['rating'].max()\n",
    "print(lower_rating,upper_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = surprise.Reader(rating_scale= (.5,4))\n",
    "data = surprise.Dataset.load_from_df(dataset,reader)"
   ]
  },
  {
   "source": [
    "We will use the method SVD++, one of best performers in the Netflix challenge, which has now become a popular method for fitting recommender systems."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = surprise.SVDpp()\n",
    "output = alg.fit(data.build_full_trainset())"
   ]
  },
  {
   "source": [
    "Now we’ve fitted the model, we can check the predicted score of, for example, user 50 on a music artist 52 using the predict method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3.0028030537791928"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "pred = alg.predict(uid='50',iid = '52')\n",
    "score = pred.est\n",
    "score"
   ]
  },
  {
   "source": [
    "## Making Recommendations\n",
    "\n",
    "Let’s make our recommendations to a particular user. Let’s focus on uid 50 and find one item to recommend them. First we need to find the movie ids that user 50 didn’t rate, since we don’t want to recommend them a movie they’ve already watched!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "iids = dataset['iid'].unique() # series of all movies ids\n",
    "iids50 = dataset.loc[dataset['uid']==50,'iid'] # rated movies by user\n",
    "iids_to_pred = np.setdiff1d(iids,iids50) #remove iids that uid 50 has rated from the list"
   ]
  },
  {
   "source": [
    "Next we want to predict the score of each of the movie ids that user 50 didn’t rate, and find the best one. For this we have to create another dataset with the iids we want to predict in the sparse format as before of: uid, iid, rating. We'll just arbitrarily set all the ratings of this test set to 4, as they are not needed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Prediction(uid=50, iid=14, r_ui=4.0, est=3.1756350972746894, details={'was_impossible': False})"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "testset = [[50,iid,4.] for iid in iids_to_pred]\n",
    "predictions = alg.test(testset)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ratings = np.array([pred.est for pred in predictions])\n",
    "i_max = pred_ratings.argmax()\n",
    "\n",
    "iid = iids_to_pred[i_max]"
   ]
  },
  {
   "source": [
    "## Tuning and Evaluating the Model\n",
    "\n",
    "As you probably already know, it is bad practice to fit a model on the whole dataset without checking its performance and tuning parameters which affect the fit. So for the remainder of the tutorial we’ll show you how to tune the parameters of SVD++ and evaluate the performance of the method. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'lr_all':[.001,0.1], 'reg_all':[.1,.5]}\n",
    "gs = surprise.model_selection.GridSearchCV(surprise.SVDpp,param_grid,measures=['rmse','mae'], cv =3)\n",
    "gs.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'lr_all': 0.1, 'reg_all': 0.1}"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "gs.best_params['rmse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVDpp on 5 split(s).\n\n                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \nRMSE (testset)    0.8186  0.8118  0.8435  0.8290  0.8355  0.8277  0.0114  \nMAE (testset)     0.6493  0.6439  0.6656  0.6542  0.6630  0.6552  0.0082  \nFit time          11.18   11.51   11.17   11.28   11.12   11.25   0.14    \nTest time         0.39    0.38    0.40    0.77    0.38    0.46    0.15    \n"
     ]
    }
   ],
   "source": [
    "alg = surprise.SVDpp(lr_all=.001)\n",
    "output = surprise.model_selection.cross_validate(alg,data,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}