{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnnscrapercondaa354d8fb32614818906b42d96b0e4fbd",
   "display_name": "Python 3.8.5 64-bit ('NNScraper': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Bag of Words Model\n",
    "Find the unique words i.e., vocabulary from the list of documents. Parse each document word with the vocabulary, if present ‘1’ else ‘0’. This makes each document vector maintain the same length that of vocabulary length. We use this vocabulary for the new document vectorization."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vocabulary:  ['superb,', 'this', 'love', 'am', 'i', 'hate', 'phone', 'in']\nvectors:  [[1, 1, 1, 1, 1, 0, 1, 1], [0, 1, 0, 0, 1, 1, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "docs = [\"SUPERB, I AM IN LOVE IN THIS PHONE\", \"I hate this phone\"]\n",
    "words = list(set([word for doc in docs for word in doc.lower().split()]))\n",
    "vectors = []\n",
    "for doc in docs:\n",
    "    vectors.append([1 if word in doc.lower().split() else 0 for word in words])\n",
    "print(\"vocabulary: \", words)   \n",
    "print(\"vectors: \", vectors)"
   ]
  },
  {
   "source": [
    "## Word Counts with CountVectorizer(scikit-learn)\n",
    " Tokenize the collection of documents and form a vocabulary with it and use this vocabulary to encode new documents. We can use CountVectorizer of the scikit-learn library. It by default remove punctuation and lower the documents."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vocabulary:  {'superb': 5, 'am': 0, 'in': 2, 'love': 3, 'this': 6, 'phone': 4, 'hate': 1}\nshape:  (2, 7)\nvectors:  [[1 0 2 1 1 1 1]\n [0 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#!pip install sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer# list of documents\n",
    "docs = ['SUPERB, I AM IN LOVE IN THIS PHONE', 'I hate this phone']# create the transform\n",
    "vectorizer = CountVectorizer()# tokenize and build vocab\n",
    "vectorizer.fit(docs)\n",
    "print('vocabulary: ', vectorizer.vocabulary_)# encode document\n",
    "vector = vectorizer.transform(docs)# summarize encoded vector\n",
    "print('shape: ', vector.shape)\n",
    "print('vectors: ', vector.toarray())"
   ]
  },
  {
   "source": [
    "##  Word Frequencies with TfidfVectorizer (scikit-learn) \n",
    "Word counts are pretty basic. In the first document, the word “in” has repeated and with that word we can’t draw any meaning. Stop words can repeat several times in a document and word count prioritize with the occurrence of the word. From word counts, we lose the interesting words and we mostly give priority to stopping words/less meaning carrying words."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vocabulary:  {'superb': 5, 'am': 0, 'in': 2, 'love': 3, 'this': 6, 'phone': 4, 'hate': 1}\nidfs:  [1.40546511 1.40546511 1.40546511 1.40546511 1.         1.40546511\n 1.        ]\nvectors:  [[0.35327777 0.         0.70655553 0.35327777 0.25136004 0.35327777\n  0.25136004]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer# list of documents\n",
    "docs = [\"SUPERB, I AM IN LOVE IN THIS PHONE\", \n",
    "        \"I hate this phone\"]# create the transform\n",
    "vectorizer = TfidfVectorizer()# tokenize and build vocab\n",
    "vectorizer.fit(docs)# summarize\n",
    "print('vocabulary: ', vectorizer.vocabulary_)\n",
    "print('idfs: ', vectorizer.idf_)# encode document\n",
    "vector = vectorizer.transform([docs[0]])# summarize encoded vector\n",
    "print('vectors: ', vector.toarray())"
   ]
  },
  {
   "source": [
    "Tf-idf is the best vectorization method among these three because it prioritises the words in each document. IDF value for the word “this” is less since it present in both the documents. So, unlike word counts which give a higher value for stop words like “in”, “this”, word frequency lowers the value if it present in more number of documents, because stop words repeat in each document almost."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}